# PDF Ingestion Strategies for LLM Applications

## Introduction

As Large Language Models (LLMs) are increasingly used for question answering and summarization over documents, the method of ingesting documents plays a critical role in performance. Current pipelines like Reducto employ structured chunk-based ingestion, where documents are parsed into semantically meaningful blocks or sections. However, alternative ingestion strategies – such as fixed-size rolling windows, sentence-level overlap, or adaptive chunking based on token count – offer different trade-offs in context fidelity, retrieval accuracy, and factual correctness.

This research aims to systematically compare multiple PDF ingestion strategies in terms of their effectiveness for downstream LLM tasks, including retrieval-augmented answering and factual consistency. The core question is: how do different chunking and ingestion methods impact the accuracy, coverage, and factuality of LLM-generated outputs?

## Structured Chunking (Reducto)

Reducto produces semantically coherent chunks that preserve the original document's layout and logical structure (e.g., sections, tables, headers), which improves both keyword-based and contextual searches. In a typical setup, the LLM can query the chunked document using a structured schema with the following parameters:

- file_name: the document to search
- chunk_index: a specific chunk to retrieve
- search_text: keyword(s) to match within chunks
- page_range: a defined span of pages to narrow the search
- limit: the maximum number of chunks to return

When the LLM is given autonomy to mix keyword search with targeted chunk retrieval, it consistently surfaces highly relevant segments—even in long and complex PDFs. This highlights the strength of Reducto's chunking for downstream QA and summarization tasks, especially in comparison to naive fixed-size or overlapping windows which often split meaningful content mid-thought.

## Fixed-Size Rolling Windows

Fixed-size rolling windows divide the document into chunks of a predetermined size (e.g., 500 tokens) with a configurable stride (e.g., 250 tokens) that determines the overlap between consecutive chunks. This approach is simple to implement and ensures consistent chunk sizes, which can be beneficial for LLMs with fixed context windows.

However, fixed-size chunking often breaks semantic boundaries, potentially splitting paragraphs, sentences, or even words across different chunks. This can lead to loss of context and reduced retrieval effectiveness, especially for queries that depend on understanding the logical structure of the document.

## Sentence-Level Overlap Chunking

Sentence-level overlap chunking respects sentence boundaries while creating chunks with a specified number of sentences and overlap. For example, a configuration might include 10 sentences per chunk with a 3-sentence overlap between consecutive chunks.

This approach preserves the integrity of sentences, which can improve the coherence of retrieved content. The overlap helps maintain context across chunk boundaries, potentially improving retrieval for queries that span multiple chunks.

## Evaluation Framework

Evaluating PDF ingestion strategies requires a comprehensive framework that considers multiple dimensions:

1. Retrieval Quality: Using metrics like precision, recall, and F1 score to assess if relevant chunks are surfaced for a given query.

2. LLM Answer Quality: Judging the quality of answers generated by an LLM based on retrieved chunks, using metrics like relevance, completeness, and coherence.

3. Factual Consistency: Measuring the factual accuracy of generated answers compared to the source document, using techniques like fact verification and entailment detection.

Traditional IR metrics like precision or recall measure retrieval accuracy but fail to capture nuanced effects on LLM output quality – such as completeness, contextual accuracy, or the likelihood of hallucinations. Similarly, summary metrics like ROUGE or BLEU are insufficiently sensitive to the structure and relevance of source chunks.

## Experimental Setup

To compare the different ingestion strategies, we will:

1. Implement each strategy (Reducto-style structured chunking, fixed-size rolling windows, sentence-level overlap chunking)
2. Process the same set of PDF documents through each method
3. Generate a set of benchmark queries covering different types of information needs
4. Evaluate each strategy using our evaluation framework
5. Analyze the results to identify trade-offs and recommend best practices

For image-rich PDFs, we will use OpenAI's CLIP (Contrastive Language-Image Pre-training) to create structured data that an LLM can understand, enhancing contextual understanding of images and information flow.

## Conclusion

The choice of PDF ingestion strategy significantly impacts the performance of LLM-based document question answering and summarization systems. By systematically comparing different approaches, we can identify the strengths and weaknesses of each strategy and provide recommendations for optimal document processing in LLM pipelines.

This research contributes to the growing body of knowledge on retrieval-augmented generation and helps establish best practices for document ingestion in LLM applications.
