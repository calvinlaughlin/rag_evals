"""factual_consistency.py
Evaluate factual consistency of GPT answers using a local NLI model.

Pipeline
--------
1. Load answers generated by ``answer_eval_results.json`` (or run
   ``llm_evaluation.py`` first).
2. For each answer, break it into sentences (facts).
3. Use a transformers NLI model (roberta-large-mnli) to test whether each
   sentence is entailed by the concatenated context chunks.
4. Compute metrics: supported_ratio, avg_confidence, consistency_score.
5. Aggregate by strategy and output a bar chart.

This avoids additional OpenAI calls and runs locally.
"""
from __future__ import annotations

import argparse
import json
import os
from typing import Dict, Any, List

import nltk
from nltk.tokenize import sent_tokenize

import torch
from transformers import AutoModelForSequenceClassification, AutoTokenizer
import matplotlib.pyplot as plt

nltk.download('punkt', quiet=True)

MODEL_NAME = "roberta-large-mnli"

print("Loading NLI model... (this may take a while the first time)")
_tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
_model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)
_model.eval()
_device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
_model.to(_device)

LABEL2ID = {0: "contradiction", 1: "neutral", 2: "entailment"}


def _nli(premise: str, hypothesis: str) -> float:
    """Return entailment probability of *hypothesis* given *premise*."""
    inputs = _tokenizer(premise, hypothesis, return_tensors="pt", truncation=True, max_length=512).to(_device)
    with torch.no_grad():
        logits = _model(**inputs).logits
        probs = logits.softmax(dim=-1)[0]
    return float(probs[2])  # entailment prob


def evaluate_answer(answer: str, context: str) -> Dict[str, Any]:
    sentences = [s.strip() for s in sent_tokenize(answer) if s.strip()]
    if not sentences:
        return {"supported_ratio": 0.0, "avg_confidence": 0.0, "details": []}

    details = []
    supported = 0
    confidences: List[float] = []
    for sent in sentences:
        conf = _nli(context, sent)
        confidences.append(conf)
        if conf > 0.5:  # threshold for entailment
            supported += 1
        details.append({"sentence": sent, "entailment_prob": conf})

    supported_ratio = supported / len(sentences)
    avg_conf = sum(confidences) / len(confidences)
    consistency = 0.7 * supported_ratio + 0.3 * avg_conf
    return {
        "supported_ratio": supported_ratio,
        "avg_confidence": avg_conf,
        "consistency_score": consistency,
        "details": details,
    }


def main() -> None:
    ap = argparse.ArgumentParser(description="Factual consistency check using MNLI model")
    ap.add_argument("answer_json", default="answer_eval_results.json", nargs="?", help="JSON with answers from llm_evaluation.py")
    ap.add_argument("--output", default="factual_consistency_results.json")
    args = ap.parse_args()

    with open(args.answer_json) as fh:
        answers: Dict[str, Dict[str, Any]] = json.load(fh)

    consistency: Dict[str, Any] = {}
    for strat, qdict in answers.items():
        strat_stats = {"scores": []}
        for q, data in qdict.items():
            ctx = "\n\n".join(c["content"] for c in data["chunks"])
            metrics = evaluate_answer(data["answer"], ctx)
            strat_stats[q] = metrics
            strat_stats["scores"].append(metrics["consistency_score"])
        consistency[strat] = strat_stats
        consistency[strat]["avg"] = sum(strat_stats["scores"]) / len(strat_stats["scores"])

    with open(args.output, "w") as fh:
        json.dump(consistency, fh, indent=2)
    print(f"Saved results to {args.output}")

    # Plot
    plt.figure(figsize=(8, 5))
    strategies = list(consistency.keys())
    scores = [consistency[s]["avg"] for s in strategies]
    plt.bar(range(len(scores)), scores, color="steelblue")
    plt.ylabel("Consistency Score")
    plt.xticks(range(len(scores)), [s.split("(")[0].strip() for s in strategies], rotation=45, ha="right")
    plt.title("Factual Consistency by Chunking Strategy")
    plt.tight_layout()
    plt.savefig("factual_consistency_comparison.png")
    print("Plot saved as factual_consistency_comparison.png")


if __name__ == "__main__":
    main()
