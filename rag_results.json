{
  "Structured Chunking (Reducto-style)": {
    "What is Reducto?": {
      "answer": "Reducto is a method that produces semantically coherent chunks from documents, preserving the original layout and logical structure to improve keyword-based and contextual searches.",
      "chunks": [
        {
          "content": "## Structured Chunking (Reducto)\nReducto produces semantically coherent chunks that preserve the original document's layout and logical structure (e.g., sections, tables, headers), which improves both keyword-based and contextual searches. In a typical setup, the LLM can query the chunked document using a structured schema with the following parameters:\n- file_name: the document to search\n- chunk_index: a specific chunk to retrieve\n- search_text: keyword(s) to match within chunks\n- page_range: a defined span of pages to narrow the search\n- limit: the maximum number of chunks to return\nWhen the LLM is given autonomy to mix keyword search with targeted chunk retrieval, it consistently surfaces highly relevant segments\u2014even in long and complex PDFs. This highlights the strength of Reducto's chunking for downstream QA and summarization tasks, especially in comparison to naive fixed-size or overlapping windows which often split meaningful content mid-thought.",
          "header": "Structured Chunking (Reducto)",
          "strategy": "Structured Chunking (Reducto-style)",
          "similarity": 0.3406890630722046
        },
        {
          "content": "## Introduction\nAs Large Language Models (LLMs) are increasingly used for question answering and summarization over documents, the method of ingesting documents plays a critical role in performance. Current pipelines like Reducto employ structured chunk-based ingestion, where documents are parsed into semantically meaningful blocks or sections. However, alternative ingestion strategies \u2013 such as fixed-size rolling windows, sentence-level overlap, or adaptive chunking based on token count \u2013 offer different trade-offs in context fidelity, retrieval accuracy, and factual correctness.\nThis research aims to systematically compare multiple PDF ingestion strategies in terms of their effectiveness for downstream LLM tasks, including retrieval-augmented answering and factual consistency. The core question is: how do different chunking and ingestion methods impact the accuracy, coverage, and factuality of LLM-generated outputs?",
          "header": "Introduction",
          "strategy": "Structured Chunking (Reducto-style)",
          "similarity": 0.15141397714614868
        },
        {
          "content": "## Experimental Setup\nTo compare the different ingestion strategies, we will:\n1. Implement each strategy (Reducto-style structured chunking, fixed-size rolling windows, sentence-level overlap chunking)\n2. Process the same set of PDF documents through each method\n3. Generate a set of benchmark queries covering different types of information needs\n4. Evaluate each strategy using our evaluation framework\n5. Analyze the results to identify trade-offs and recommend best practices\nFor image-rich PDFs, we will use OpenAI's CLIP (Contrastive Language-Image Pre-training) to create structured data that an LLM can understand, enhancing contextual understanding of images and information flow.",
          "header": "Experimental Setup",
          "strategy": "Structured Chunking (Reducto-style)",
          "similarity": 0.06292671710252762
        },
        {
          "content": "## Evaluation Framework",
          "header": "Evaluation Framework",
          "strategy": "Structured Chunking (Reducto-style)",
          "similarity": 0.06087839603424072
        },
        {
          "content": "Evaluating PDF ingestion strategies requires a comprehensive framework that considers multiple dimensions:\n1. Retrieval Quality: Using metrics like precision, recall, and F1 score to assess if relevant chunks are surfaced for a given query.\n2. LLM Answer Quality: Judging the quality of answers generated by an LLM based on retrieved chunks, using metrics like relevance, completeness, and coherence.\n3. Factual Consistency: Measuring the factual accuracy of generated answers compared to the source document, using techniques like fact verification and entailment detection.\nTraditional IR metrics like precision or recall measure retrieval accuracy but fail to capture nuanced effects on LLM output quality \u2013 such as completeness, contextual accuracy, or the likelihood of hallucinations. Similarly, summary metrics like ROUGE or BLEU are insufficiently sensitive to the structure and relevance of source chunks.",
          "header": "Evaluating PDF ingestion strategies requires a comprehensive framework that considers multiple dimensions",
          "strategy": "Structured Chunking (Reducto-style)",
          "similarity": 0.05255158618092537
        }
      ],
      "usage": {
        "prompt_tokens": 725,
        "completion_tokens": 30,
        "total_tokens": 755
      }
    },
    "How does fixed-size rolling window chunking work?": {
      "answer": "Fixed-size rolling windows divide the document into chunks of a predetermined size with a configurable stride that determines the overlap between consecutive chunks.",
      "chunks": [
        {
          "content": "## Fixed-Size Rolling Windows\nFixed-size rolling windows divide the document into chunks of a predetermined size (e.g., 500 tokens) with a configurable stride (e.g., 250 tokens) that determines the overlap between consecutive chunks. This approach is simple to implement and ensures consistent chunk sizes, which can be beneficial for LLMs with fixed context windows.\nHowever, fixed-size chunking often breaks semantic boundaries, potentially splitting paragraphs, sentences, or even words across different chunks. This can lead to loss of context and reduced retrieval effectiveness, especially for queries that depend on understanding the logical structure of the document.",
          "header": "Fixed-Size Rolling Windows",
          "strategy": "Structured Chunking (Reducto-style)",
          "similarity": 0.6553323268890381
        },
        {
          "content": "## Sentence-Level Overlap Chunking\nSentence-level overlap chunking respects sentence boundaries while creating chunks with a specified number of sentences and overlap. For example, a configuration might include 10 sentences per chunk with a 3-sentence overlap between consecutive chunks.\nThis approach preserves the integrity of sentences, which can improve the coherence of retrieved content. The overlap helps maintain context across chunk boundaries, potentially improving retrieval for queries that span multiple chunks.",
          "header": "Sentence-Level Overlap Chunking",
          "strategy": "Structured Chunking (Reducto-style)",
          "similarity": 0.4095507264137268
        },
        {
          "content": "## Structured Chunking (Reducto)\nReducto produces semantically coherent chunks that preserve the original document's layout and logical structure (e.g., sections, tables, headers), which improves both keyword-based and contextual searches. In a typical setup, the LLM can query the chunked document using a structured schema with the following parameters:\n- file_name: the document to search\n- chunk_index: a specific chunk to retrieve\n- search_text: keyword(s) to match within chunks\n- page_range: a defined span of pages to narrow the search\n- limit: the maximum number of chunks to return\nWhen the LLM is given autonomy to mix keyword search with targeted chunk retrieval, it consistently surfaces highly relevant segments\u2014even in long and complex PDFs. This highlights the strength of Reducto's chunking for downstream QA and summarization tasks, especially in comparison to naive fixed-size or overlapping windows which often split meaningful content mid-thought.",
          "header": "Structured Chunking (Reducto)",
          "strategy": "Structured Chunking (Reducto-style)",
          "similarity": 0.32912477850914
        },
        {
          "content": "## Introduction\nAs Large Language Models (LLMs) are increasingly used for question answering and summarization over documents, the method of ingesting documents plays a critical role in performance. Current pipelines like Reducto employ structured chunk-based ingestion, where documents are parsed into semantically meaningful blocks or sections. However, alternative ingestion strategies \u2013 such as fixed-size rolling windows, sentence-level overlap, or adaptive chunking based on token count \u2013 offer different trade-offs in context fidelity, retrieval accuracy, and factual correctness.\nThis research aims to systematically compare multiple PDF ingestion strategies in terms of their effectiveness for downstream LLM tasks, including retrieval-augmented answering and factual consistency. The core question is: how do different chunking and ingestion methods impact the accuracy, coverage, and factuality of LLM-generated outputs?",
          "header": "Introduction",
          "strategy": "Structured Chunking (Reducto-style)",
          "similarity": 0.26830247044563293
        },
        {
          "content": "## Experimental Setup\nTo compare the different ingestion strategies, we will:\n1. Implement each strategy (Reducto-style structured chunking, fixed-size rolling windows, sentence-level overlap chunking)\n2. Process the same set of PDF documents through each method\n3. Generate a set of benchmark queries covering different types of information needs\n4. Evaluate each strategy using our evaluation framework\n5. Analyze the results to identify trade-offs and recommend best practices\nFor image-rich PDFs, we will use OpenAI's CLIP (Contrastive Language-Image Pre-training) to create structured data that an LLM can understand, enhancing contextual understanding of images and information flow.",
          "header": "Experimental Setup",
          "strategy": "Structured Chunking (Reducto-style)",
          "similarity": 0.18764030933380127
        }
      ],
      "usage": {
        "prompt_tokens": 768,
        "completion_tokens": 25,
        "total_tokens": 793
      }
    }
  },
  "Fixed-Size Rolling Window (size=500, stride=250)": {
    "What is Reducto?": {
      "answer": "Reducto is a method that produces semantically coherent chunks from documents, preserving the original document's layout and logical structure.",
      "chunks": [
        {
          "content": "# PDF Ingestion Strategies for LLM Applications ## Introduction As Large Language Models (LLMs) are increasingly used for question answering and summarization over documents, the method of ingesting documents plays a critical role in performance. Current pipelines like Reducto employ structured chunk-based ingestion, where documents are parsed into semantically meaningful blocks or sections. However, alternative ingestion strategies \u2013 such as fixed-size rolling windows, sentence-level overlap, or adaptive chunking based on token count \u2013 offer different trade-offs in context fidelity, retrieval accuracy, and factual correctness. This research aims to systematically compare multiple PDF ingestion strategies in terms of their effectiveness for downstream LLM tasks, including retrieval-augmented answering and factual consistency. The core question is: how do different chunking and ingestion methods impact the accuracy, coverage, and factuality of LLM-generated outputs? ## Structured Chunking (Reducto) Reducto produces semantically coherent chunks that preserve the original document's layout and logical structure (e.g., sections, tables, headers), which improves both keyword-based and contextual searches. In a typical setup, the LLM can query the chunked document using a structured schema with the following parameters: - file_name: the document to search - chunk_index: a specific chunk to retrieve - search_text: keyword(s) to match within chunks - page_range: a defined span of pages to narrow the search - limit: the maximum number of chunks to return When the LLM is given autonomy to mix keyword search with targeted chunk retrieval, it consistently surfaces highly relevant segments\u2014even in long and complex PDFs. This highlights the strength of Reducto's chunking for downstream QA and summarization tasks, especially in comparison to naive fixed-size or overlapping windows which often split meaningful content mid-thought. ## Fixed-Size Rolling Windows Fixed-size rolling windows divide the document into chunks of a predetermined size (e.g., 500 tokens) with a configurable stride (e.g., 250 tokens) that determines the overlap between consecutive chunks. This approach is simple to implement and ensures consistent chunk sizes, which can be beneficial for LLMs with fixed context windows. However, fixed-size chunking often breaks semantic boundaries, potentially splitting paragraphs, sentences, or even words across different chunks. This can lead to loss of context and reduced retrieval effectiveness, especially for queries that depend on understanding the logical structure of the document. ## Sentence-Level Overlap Chunking Sentence-level overlap chunking respects sentence boundaries while creating chunks with a specified number of sentences and overlap. For example, a configuration might include 10 sentences per chunk with a 3-sentence overlap between consecutive chunks. This approach preserves the integrity of sentences, which can improve the coherence of retrieved content. The overlap helps maintain context across chunk boundaries, potentially improving retrieval for queries that span multiple chunks. ## Evaluation Framework Evaluating PDF ingestion strategies requires a comprehensive framework that considers multiple dimensions: 1. Retrieval Quality: Using metrics like precision, recall, and F1 score to assess if relevant chunks are surfaced for a given query. 2. LLM Answer Quality: Judging the quality of answers generated by an LLM based on retrieved chunks, using metrics like relevance, completeness, and coherence. 3. Factual Consistency: Measuring",
          "start_idx": 0,
          "end_idx": 500,
          "strategy": "Fixed-Size Rolling Window (size=500, stride=250)",
          "similarity": 0.1924300193786621
        },
        {
          "content": "the factual accuracy of generated answers compared to the source document, using techniques like fact verification and entailment detection. Traditional IR metrics like precision or recall measure retrieval accuracy but fail to capture nuanced effects on LLM output quality \u2013 such as completeness, contextual accuracy, or the likelihood of hallucinations. Similarly, summary metrics like ROUGE or BLEU are insufficiently sensitive to the structure and relevance of source chunks. ## Experimental Setup To compare the different ingestion strategies, we will: 1. Implement each strategy (Reducto-style structured chunking, fixed-size rolling windows, sentence-level overlap chunking) 2. Process the same set of PDF documents through each method 3. Generate a set of benchmark queries covering different types of information needs 4. Evaluate each strategy using our evaluation framework 5. Analyze the results to identify trade-offs and recommend best practices For image-rich PDFs, we will use OpenAI's CLIP (Contrastive Language-Image Pre-training) to create structured data that an LLM can understand, enhancing contextual understanding of images and information flow. ## Conclusion The choice of PDF ingestion strategy significantly impacts the performance of LLM-based document question answering and summarization systems. By systematically comparing different approaches, we can identify the strengths and weaknesses of each strategy and provide recommendations for optimal document processing in LLM pipelines. This research contributes to the growing body of knowledge on retrieval-augmented generation and helps establish best practices for document ingestion in LLM applications.",
          "start_idx": 500,
          "end_idx": 731,
          "strategy": "Fixed-Size Rolling Window (size=500, stride=250)",
          "similarity": 0.06681874394416809
        },
        {
          "content": "downstream QA and summarization tasks, especially in comparison to naive fixed-size or overlapping windows which often split meaningful content mid-thought. ## Fixed-Size Rolling Windows Fixed-size rolling windows divide the document into chunks of a predetermined size (e.g., 500 tokens) with a configurable stride (e.g., 250 tokens) that determines the overlap between consecutive chunks. This approach is simple to implement and ensures consistent chunk sizes, which can be beneficial for LLMs with fixed context windows. However, fixed-size chunking often breaks semantic boundaries, potentially splitting paragraphs, sentences, or even words across different chunks. This can lead to loss of context and reduced retrieval effectiveness, especially for queries that depend on understanding the logical structure of the document. ## Sentence-Level Overlap Chunking Sentence-level overlap chunking respects sentence boundaries while creating chunks with a specified number of sentences and overlap. For example, a configuration might include 10 sentences per chunk with a 3-sentence overlap between consecutive chunks. This approach preserves the integrity of sentences, which can improve the coherence of retrieved content. The overlap helps maintain context across chunk boundaries, potentially improving retrieval for queries that span multiple chunks. ## Evaluation Framework Evaluating PDF ingestion strategies requires a comprehensive framework that considers multiple dimensions: 1. Retrieval Quality: Using metrics like precision, recall, and F1 score to assess if relevant chunks are surfaced for a given query. 2. LLM Answer Quality: Judging the quality of answers generated by an LLM based on retrieved chunks, using metrics like relevance, completeness, and coherence. 3. Factual Consistency: Measuring the factual accuracy of generated answers compared to the source document, using techniques like fact verification and entailment detection. Traditional IR metrics like precision or recall measure retrieval accuracy but fail to capture nuanced effects on LLM output quality \u2013 such as completeness, contextual accuracy, or the likelihood of hallucinations. Similarly, summary metrics like ROUGE or BLEU are insufficiently sensitive to the structure and relevance of source chunks. ## Experimental Setup To compare the different ingestion strategies, we will: 1. Implement each strategy (Reducto-style structured chunking, fixed-size rolling windows, sentence-level overlap chunking) 2. Process the same set of PDF documents through each method 3. Generate a set of benchmark queries covering different types of information needs 4. Evaluate each strategy using our evaluation framework 5. Analyze the results to identify trade-offs and recommend best practices For image-rich PDFs, we will use OpenAI's CLIP (Contrastive Language-Image Pre-training) to create structured data that an LLM can understand, enhancing contextual understanding of images and information flow. ## Conclusion The choice of PDF ingestion strategy significantly impacts the performance of LLM-based document question answering and summarization systems. By systematically comparing different approaches, we can identify the strengths and weaknesses of each strategy and provide recommendations for optimal document processing in LLM pipelines. This research contributes to the growing body of knowledge on retrieval-augmented generation and helps establish best practices for document ingestion in LLM applications.",
          "start_idx": 250,
          "end_idx": 731,
          "strategy": "Fixed-Size Rolling Window (size=500, stride=250)",
          "similarity": -0.0240238718688488
        }
      ],
      "usage": {
        "prompt_tokens": 1640,
        "completion_tokens": 25,
        "total_tokens": 1665
      }
    },
    "How does fixed-size rolling window chunking work?": {
      "answer": "Fixed-size rolling windows divide the document into chunks of a predetermined size with a configurable stride that determines the overlap between consecutive chunks.",
      "chunks": [
        {
          "content": "downstream QA and summarization tasks, especially in comparison to naive fixed-size or overlapping windows which often split meaningful content mid-thought. ## Fixed-Size Rolling Windows Fixed-size rolling windows divide the document into chunks of a predetermined size (e.g., 500 tokens) with a configurable stride (e.g., 250 tokens) that determines the overlap between consecutive chunks. This approach is simple to implement and ensures consistent chunk sizes, which can be beneficial for LLMs with fixed context windows. However, fixed-size chunking often breaks semantic boundaries, potentially splitting paragraphs, sentences, or even words across different chunks. This can lead to loss of context and reduced retrieval effectiveness, especially for queries that depend on understanding the logical structure of the document. ## Sentence-Level Overlap Chunking Sentence-level overlap chunking respects sentence boundaries while creating chunks with a specified number of sentences and overlap. For example, a configuration might include 10 sentences per chunk with a 3-sentence overlap between consecutive chunks. This approach preserves the integrity of sentences, which can improve the coherence of retrieved content. The overlap helps maintain context across chunk boundaries, potentially improving retrieval for queries that span multiple chunks. ## Evaluation Framework Evaluating PDF ingestion strategies requires a comprehensive framework that considers multiple dimensions: 1. Retrieval Quality: Using metrics like precision, recall, and F1 score to assess if relevant chunks are surfaced for a given query. 2. LLM Answer Quality: Judging the quality of answers generated by an LLM based on retrieved chunks, using metrics like relevance, completeness, and coherence. 3. Factual Consistency: Measuring the factual accuracy of generated answers compared to the source document, using techniques like fact verification and entailment detection. Traditional IR metrics like precision or recall measure retrieval accuracy but fail to capture nuanced effects on LLM output quality \u2013 such as completeness, contextual accuracy, or the likelihood of hallucinations. Similarly, summary metrics like ROUGE or BLEU are insufficiently sensitive to the structure and relevance of source chunks. ## Experimental Setup To compare the different ingestion strategies, we will: 1. Implement each strategy (Reducto-style structured chunking, fixed-size rolling windows, sentence-level overlap chunking) 2. Process the same set of PDF documents through each method 3. Generate a set of benchmark queries covering different types of information needs 4. Evaluate each strategy using our evaluation framework 5. Analyze the results to identify trade-offs and recommend best practices For image-rich PDFs, we will use OpenAI's CLIP (Contrastive Language-Image Pre-training) to create structured data that an LLM can understand, enhancing contextual understanding of images and information flow. ## Conclusion The choice of PDF ingestion strategy significantly impacts the performance of LLM-based document question answering and summarization systems. By systematically comparing different approaches, we can identify the strengths and weaknesses of each strategy and provide recommendations for optimal document processing in LLM pipelines. This research contributes to the growing body of knowledge on retrieval-augmented generation and helps establish best practices for document ingestion in LLM applications.",
          "start_idx": 250,
          "end_idx": 731,
          "strategy": "Fixed-Size Rolling Window (size=500, stride=250)",
          "similarity": 0.5344571471214294
        },
        {
          "content": "# PDF Ingestion Strategies for LLM Applications ## Introduction As Large Language Models (LLMs) are increasingly used for question answering and summarization over documents, the method of ingesting documents plays a critical role in performance. Current pipelines like Reducto employ structured chunk-based ingestion, where documents are parsed into semantically meaningful blocks or sections. However, alternative ingestion strategies \u2013 such as fixed-size rolling windows, sentence-level overlap, or adaptive chunking based on token count \u2013 offer different trade-offs in context fidelity, retrieval accuracy, and factual correctness. This research aims to systematically compare multiple PDF ingestion strategies in terms of their effectiveness for downstream LLM tasks, including retrieval-augmented answering and factual consistency. The core question is: how do different chunking and ingestion methods impact the accuracy, coverage, and factuality of LLM-generated outputs? ## Structured Chunking (Reducto) Reducto produces semantically coherent chunks that preserve the original document's layout and logical structure (e.g., sections, tables, headers), which improves both keyword-based and contextual searches. In a typical setup, the LLM can query the chunked document using a structured schema with the following parameters: - file_name: the document to search - chunk_index: a specific chunk to retrieve - search_text: keyword(s) to match within chunks - page_range: a defined span of pages to narrow the search - limit: the maximum number of chunks to return When the LLM is given autonomy to mix keyword search with targeted chunk retrieval, it consistently surfaces highly relevant segments\u2014even in long and complex PDFs. This highlights the strength of Reducto's chunking for downstream QA and summarization tasks, especially in comparison to naive fixed-size or overlapping windows which often split meaningful content mid-thought. ## Fixed-Size Rolling Windows Fixed-size rolling windows divide the document into chunks of a predetermined size (e.g., 500 tokens) with a configurable stride (e.g., 250 tokens) that determines the overlap between consecutive chunks. This approach is simple to implement and ensures consistent chunk sizes, which can be beneficial for LLMs with fixed context windows. However, fixed-size chunking often breaks semantic boundaries, potentially splitting paragraphs, sentences, or even words across different chunks. This can lead to loss of context and reduced retrieval effectiveness, especially for queries that depend on understanding the logical structure of the document. ## Sentence-Level Overlap Chunking Sentence-level overlap chunking respects sentence boundaries while creating chunks with a specified number of sentences and overlap. For example, a configuration might include 10 sentences per chunk with a 3-sentence overlap between consecutive chunks. This approach preserves the integrity of sentences, which can improve the coherence of retrieved content. The overlap helps maintain context across chunk boundaries, potentially improving retrieval for queries that span multiple chunks. ## Evaluation Framework Evaluating PDF ingestion strategies requires a comprehensive framework that considers multiple dimensions: 1. Retrieval Quality: Using metrics like precision, recall, and F1 score to assess if relevant chunks are surfaced for a given query. 2. LLM Answer Quality: Judging the quality of answers generated by an LLM based on retrieved chunks, using metrics like relevance, completeness, and coherence. 3. Factual Consistency: Measuring",
          "start_idx": 0,
          "end_idx": 500,
          "strategy": "Fixed-Size Rolling Window (size=500, stride=250)",
          "similarity": 0.3047528862953186
        },
        {
          "content": "the factual accuracy of generated answers compared to the source document, using techniques like fact verification and entailment detection. Traditional IR metrics like precision or recall measure retrieval accuracy but fail to capture nuanced effects on LLM output quality \u2013 such as completeness, contextual accuracy, or the likelihood of hallucinations. Similarly, summary metrics like ROUGE or BLEU are insufficiently sensitive to the structure and relevance of source chunks. ## Experimental Setup To compare the different ingestion strategies, we will: 1. Implement each strategy (Reducto-style structured chunking, fixed-size rolling windows, sentence-level overlap chunking) 2. Process the same set of PDF documents through each method 3. Generate a set of benchmark queries covering different types of information needs 4. Evaluate each strategy using our evaluation framework 5. Analyze the results to identify trade-offs and recommend best practices For image-rich PDFs, we will use OpenAI's CLIP (Contrastive Language-Image Pre-training) to create structured data that an LLM can understand, enhancing contextual understanding of images and information flow. ## Conclusion The choice of PDF ingestion strategy significantly impacts the performance of LLM-based document question answering and summarization systems. By systematically comparing different approaches, we can identify the strengths and weaknesses of each strategy and provide recommendations for optimal document processing in LLM pipelines. This research contributes to the growing body of knowledge on retrieval-augmented generation and helps establish best practices for document ingestion in LLM applications.",
          "start_idx": 500,
          "end_idx": 731,
          "strategy": "Fixed-Size Rolling Window (size=500, stride=250)",
          "similarity": 0.13237284123897552
        }
      ],
      "usage": {
        "prompt_tokens": 1644,
        "completion_tokens": 25,
        "total_tokens": 1669
      }
    }
  },
  "Sentence Overlap Chunking (size=10, overlap=3)": {
    "What is Reducto?": {
      "answer": "Reducto is a structured chunking method that produces semantically coherent chunks preserving the original document's layout and logical structure.",
      "chunks": [
        {
          "content": "# PDF Ingestion Strategies for LLM Applications  ## Introduction  As Large Language Models (LLMs) are increasingly used for question answering and summarization over documents, the method of ingesting documents plays a critical role in performance. Current pipelines like Reducto employ structured chunk-based ingestion, where documents are parsed into semantically meaningful blocks or sections. However, alternative ingestion strategies \u2013 such as fixed-size rolling windows, sentence-level overlap, or adaptive chunking based on token count \u2013 offer different trade-offs in context fidelity, retrieval accuracy, and factual correctness. This research aims to systematically compare multiple PDF ingestion strategies in terms of their effectiveness for downstream LLM tasks, including retrieval-augmented answering and factual consistency. The core question is: how do different chunking and ingestion methods impact the accuracy, coverage, and factuality of LLM-generated outputs?  ## Structured Chunking (Reducto)  Reducto produces semantically coherent chunks that preserve the original document's layout and logical structure (e. g. , sections, tables, headers), which improves both keyword-based and contextual searches. In a typical setup, the LLM can query the chunked document using a structured schema with the following parameters:  - file_name: the document to search - chunk_index: a specific chunk to retrieve - search_text: keyword(s) to match within chunks - page_range: a defined span of pages to narrow the search - limit: the maximum number of chunks to return  When the LLM is given autonomy to mix keyword search with targeted chunk retrieval, it consistently surfaces highly relevant segments\u2014even in long and complex PDFs. This highlights the strength of Reducto's chunking for downstream QA and summarization tasks, especially in comparison to naive fixed-size or overlapping windows which often split meaningful content mid-thought. ## Fixed-Size Rolling Windows  Fixed-size rolling windows divide the document into chunks of a predetermined size (e.",
          "start_sentence": 0,
          "end_sentence": 10,
          "strategy": "Sentence Overlap Chunking (size=10, overlap=3)",
          "similarity": 0.1924300193786621
        },
        {
          "content": "In a typical setup, the LLM can query the chunked document using a structured schema with the following parameters:  - file_name: the document to search - chunk_index: a specific chunk to retrieve - search_text: keyword(s) to match within chunks - page_range: a defined span of pages to narrow the search - limit: the maximum number of chunks to return  When the LLM is given autonomy to mix keyword search with targeted chunk retrieval, it consistently surfaces highly relevant segments\u2014even in long and complex PDFs. This highlights the strength of Reducto's chunking for downstream QA and summarization tasks, especially in comparison to naive fixed-size or overlapping windows which often split meaningful content mid-thought. ## Fixed-Size Rolling Windows  Fixed-size rolling windows divide the document into chunks of a predetermined size (e. g. , 500 tokens) with a configurable stride (e. g. , 250 tokens) that determines the overlap between consecutive chunks. This approach is simple to implement and ensures consistent chunk sizes, which can be beneficial for LLMs with fixed context windows. However, fixed-size chunking often breaks semantic boundaries, potentially splitting paragraphs, sentences, or even words across different chunks. This can lead to loss of context and reduced retrieval effectiveness, especially for queries that depend on understanding the logical structure of the document.",
          "start_sentence": 7,
          "end_sentence": 17,
          "strategy": "Sentence Overlap Chunking (size=10, overlap=3)",
          "similarity": 0.1569560170173645
        },
        {
          "content": "## Evaluation Framework  Evaluating PDF ingestion strategies requires a comprehensive framework that considers multiple dimensions:  1. Retrieval Quality: Using metrics like precision, recall, and F1 score to assess if relevant chunks are surfaced for a given query. 2. LLM Answer Quality: Judging the quality of answers generated by an LLM based on retrieved chunks, using metrics like relevance, completeness, and coherence. 3. Factual Consistency: Measuring the factual accuracy of generated answers compared to the source document, using techniques like fact verification and entailment detection. Traditional IR metrics like precision or recall measure retrieval accuracy but fail to capture nuanced effects on LLM output quality \u2013 such as completeness, contextual accuracy, or the likelihood of hallucinations. Similarly, summary metrics like ROUGE or BLEU are insufficiently sensitive to the structure and relevance of source chunks. ## Experimental Setup  To compare the different ingestion strategies, we will:  1. Implement each strategy (Reducto-style structured chunking, fixed-size rolling windows, sentence-level overlap chunking) 2.",
          "start_sentence": 21,
          "end_sentence": 31,
          "strategy": "Sentence Overlap Chunking (size=10, overlap=3)",
          "similarity": 0.06411000341176987
        },
        {
          "content": "Similarly, summary metrics like ROUGE or BLEU are insufficiently sensitive to the structure and relevance of source chunks. ## Experimental Setup  To compare the different ingestion strategies, we will:  1. Implement each strategy (Reducto-style structured chunking, fixed-size rolling windows, sentence-level overlap chunking) 2. Process the same set of PDF documents through each method 3. Generate a set of benchmark queries covering different types of information needs 4. Evaluate each strategy using our evaluation framework 5. Analyze the results to identify trade-offs and recommend best practices  For image-rich PDFs, we will use OpenAI's CLIP (Contrastive Language-Image Pre-training) to create structured data that an LLM can understand, enhancing contextual understanding of images and information flow. ## Conclusion  The choice of PDF ingestion strategy significantly impacts the performance of LLM-based document question answering and summarization systems. By systematically comparing different approaches, we can identify the strengths and weaknesses of each strategy and provide recommendations for optimal document processing in LLM pipelines. This research contributes to the growing body of knowledge on retrieval-augmented generation and helps establish best practices for document ingestion in LLM applications.",
          "start_sentence": 28,
          "end_sentence": 38,
          "strategy": "Sentence Overlap Chunking (size=10, overlap=3)",
          "similarity": 0.033380694687366486
        },
        {
          "content": "This approach is simple to implement and ensures consistent chunk sizes, which can be beneficial for LLMs with fixed context windows. However, fixed-size chunking often breaks semantic boundaries, potentially splitting paragraphs, sentences, or even words across different chunks. This can lead to loss of context and reduced retrieval effectiveness, especially for queries that depend on understanding the logical structure of the document. ## Sentence-Level Overlap Chunking  Sentence-level overlap chunking respects sentence boundaries while creating chunks with a specified number of sentences and overlap. For example, a configuration might include 10 sentences per chunk with a 3-sentence overlap between consecutive chunks. This approach preserves the integrity of sentences, which can improve the coherence of retrieved content. The overlap helps maintain context across chunk boundaries, potentially improving retrieval for queries that span multiple chunks. ## Evaluation Framework  Evaluating PDF ingestion strategies requires a comprehensive framework that considers multiple dimensions:  1. Retrieval Quality: Using metrics like precision, recall, and F1 score to assess if relevant chunks are surfaced for a given query. 2.",
          "start_sentence": 14,
          "end_sentence": 24,
          "strategy": "Sentence Overlap Chunking (size=10, overlap=3)",
          "similarity": -0.019236918538808823
        }
      ],
      "usage": {
        "prompt_tokens": 1399,
        "completion_tokens": 25,
        "total_tokens": 1424
      }
    },
    "How does fixed-size rolling window chunking work?": {
      "answer": "Fixed-size rolling windows divide the document into chunks of a predetermined size with a configurable stride that determines the overlap between consecutive chunks.",
      "chunks": [
        {
          "content": "In a typical setup, the LLM can query the chunked document using a structured schema with the following parameters:  - file_name: the document to search - chunk_index: a specific chunk to retrieve - search_text: keyword(s) to match within chunks - page_range: a defined span of pages to narrow the search - limit: the maximum number of chunks to return  When the LLM is given autonomy to mix keyword search with targeted chunk retrieval, it consistently surfaces highly relevant segments\u2014even in long and complex PDFs. This highlights the strength of Reducto's chunking for downstream QA and summarization tasks, especially in comparison to naive fixed-size or overlapping windows which often split meaningful content mid-thought. ## Fixed-Size Rolling Windows  Fixed-size rolling windows divide the document into chunks of a predetermined size (e. g. , 500 tokens) with a configurable stride (e. g. , 250 tokens) that determines the overlap between consecutive chunks. This approach is simple to implement and ensures consistent chunk sizes, which can be beneficial for LLMs with fixed context windows. However, fixed-size chunking often breaks semantic boundaries, potentially splitting paragraphs, sentences, or even words across different chunks. This can lead to loss of context and reduced retrieval effectiveness, especially for queries that depend on understanding the logical structure of the document.",
          "start_sentence": 7,
          "end_sentence": 17,
          "strategy": "Sentence Overlap Chunking (size=10, overlap=3)",
          "similarity": 0.4541085660457611
        },
        {
          "content": "This approach is simple to implement and ensures consistent chunk sizes, which can be beneficial for LLMs with fixed context windows. However, fixed-size chunking often breaks semantic boundaries, potentially splitting paragraphs, sentences, or even words across different chunks. This can lead to loss of context and reduced retrieval effectiveness, especially for queries that depend on understanding the logical structure of the document. ## Sentence-Level Overlap Chunking  Sentence-level overlap chunking respects sentence boundaries while creating chunks with a specified number of sentences and overlap. For example, a configuration might include 10 sentences per chunk with a 3-sentence overlap between consecutive chunks. This approach preserves the integrity of sentences, which can improve the coherence of retrieved content. The overlap helps maintain context across chunk boundaries, potentially improving retrieval for queries that span multiple chunks. ## Evaluation Framework  Evaluating PDF ingestion strategies requires a comprehensive framework that considers multiple dimensions:  1. Retrieval Quality: Using metrics like precision, recall, and F1 score to assess if relevant chunks are surfaced for a given query. 2.",
          "start_sentence": 14,
          "end_sentence": 24,
          "strategy": "Sentence Overlap Chunking (size=10, overlap=3)",
          "similarity": 0.3951260447502136
        },
        {
          "content": "# PDF Ingestion Strategies for LLM Applications  ## Introduction  As Large Language Models (LLMs) are increasingly used for question answering and summarization over documents, the method of ingesting documents plays a critical role in performance. Current pipelines like Reducto employ structured chunk-based ingestion, where documents are parsed into semantically meaningful blocks or sections. However, alternative ingestion strategies \u2013 such as fixed-size rolling windows, sentence-level overlap, or adaptive chunking based on token count \u2013 offer different trade-offs in context fidelity, retrieval accuracy, and factual correctness. This research aims to systematically compare multiple PDF ingestion strategies in terms of their effectiveness for downstream LLM tasks, including retrieval-augmented answering and factual consistency. The core question is: how do different chunking and ingestion methods impact the accuracy, coverage, and factuality of LLM-generated outputs?  ## Structured Chunking (Reducto)  Reducto produces semantically coherent chunks that preserve the original document's layout and logical structure (e. g. , sections, tables, headers), which improves both keyword-based and contextual searches. In a typical setup, the LLM can query the chunked document using a structured schema with the following parameters:  - file_name: the document to search - chunk_index: a specific chunk to retrieve - search_text: keyword(s) to match within chunks - page_range: a defined span of pages to narrow the search - limit: the maximum number of chunks to return  When the LLM is given autonomy to mix keyword search with targeted chunk retrieval, it consistently surfaces highly relevant segments\u2014even in long and complex PDFs. This highlights the strength of Reducto's chunking for downstream QA and summarization tasks, especially in comparison to naive fixed-size or overlapping windows which often split meaningful content mid-thought. ## Fixed-Size Rolling Windows  Fixed-size rolling windows divide the document into chunks of a predetermined size (e.",
          "start_sentence": 0,
          "end_sentence": 10,
          "strategy": "Sentence Overlap Chunking (size=10, overlap=3)",
          "similarity": 0.3047528862953186
        },
        {
          "content": "Similarly, summary metrics like ROUGE or BLEU are insufficiently sensitive to the structure and relevance of source chunks. ## Experimental Setup  To compare the different ingestion strategies, we will:  1. Implement each strategy (Reducto-style structured chunking, fixed-size rolling windows, sentence-level overlap chunking) 2. Process the same set of PDF documents through each method 3. Generate a set of benchmark queries covering different types of information needs 4. Evaluate each strategy using our evaluation framework 5. Analyze the results to identify trade-offs and recommend best practices  For image-rich PDFs, we will use OpenAI's CLIP (Contrastive Language-Image Pre-training) to create structured data that an LLM can understand, enhancing contextual understanding of images and information flow. ## Conclusion  The choice of PDF ingestion strategy significantly impacts the performance of LLM-based document question answering and summarization systems. By systematically comparing different approaches, we can identify the strengths and weaknesses of each strategy and provide recommendations for optimal document processing in LLM pipelines. This research contributes to the growing body of knowledge on retrieval-augmented generation and helps establish best practices for document ingestion in LLM applications.",
          "start_sentence": 28,
          "end_sentence": 38,
          "strategy": "Sentence Overlap Chunking (size=10, overlap=3)",
          "similarity": 0.21802112460136414
        },
        {
          "content": "## Evaluation Framework  Evaluating PDF ingestion strategies requires a comprehensive framework that considers multiple dimensions:  1. Retrieval Quality: Using metrics like precision, recall, and F1 score to assess if relevant chunks are surfaced for a given query. 2. LLM Answer Quality: Judging the quality of answers generated by an LLM based on retrieved chunks, using metrics like relevance, completeness, and coherence. 3. Factual Consistency: Measuring the factual accuracy of generated answers compared to the source document, using techniques like fact verification and entailment detection. Traditional IR metrics like precision or recall measure retrieval accuracy but fail to capture nuanced effects on LLM output quality \u2013 such as completeness, contextual accuracy, or the likelihood of hallucinations. Similarly, summary metrics like ROUGE or BLEU are insufficiently sensitive to the structure and relevance of source chunks. ## Experimental Setup  To compare the different ingestion strategies, we will:  1. Implement each strategy (Reducto-style structured chunking, fixed-size rolling windows, sentence-level overlap chunking) 2.",
          "start_sentence": 21,
          "end_sentence": 31,
          "strategy": "Sentence Overlap Chunking (size=10, overlap=3)",
          "similarity": 0.14080096781253815
        }
      ],
      "usage": {
        "prompt_tokens": 1403,
        "completion_tokens": 25,
        "total_tokens": 1428
      }
    }
  }
}